{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# HYBRID SEQ2SEQ + EXTRACTIVE EVAL (KAGGLE T4 READY - FINAL)\n# =============================================================================\n\n!pip install datasets rouge-score nltk torch -q\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom rouge_score import rouge_scorer\nfrom datasets import load_dataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport nltk\nimport numpy as np\nfrom collections import Counter\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nnltk.download(\"punkt\", quiet=True)\nprint(\"âœ“ Setup complete\")\n\n# =============================================================================\n# LOAD DATASET\n# =============================================================================\nprint(\"\\nLoading dataset...\")\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\ntrain_data = dataset[\"train\"].select(range(15000))\nval_data = dataset[\"validation\"].select(range(2000))\nprint(\"âœ“ Dataset loaded\")\n\n# =============================================================================\n# EXTRACTIVE PREPROCESSOR\n# =============================================================================\nclass ExtractivePreprocessor:\n    def pure_extractive_summary(self, text, max_sents=3):\n        \"\"\"Lead-3 baseline for ROUGE evaluation\"\"\"\n        try:\n            sentences = nltk.sent_tokenize(text)\n        except:\n            sentences = text.split(\".\")\n        return \" \".join(sentences[:max_sents]) if sentences else text\n\n    def extract_important_sentences(self, text, max_sentences=6):\n        \"\"\"Extract sentences for model input\"\"\"\n        try:\n            sentences = nltk.sent_tokenize(text)\n        except:\n            sentences = text.split(\".\")\n        return \" \".join(sentences[:max_sentences]) if sentences else text\n\nextractor = ExtractivePreprocessor()\n\n# =============================================================================\n# TOKENIZER\n# =============================================================================\nclass HybridTokenizer:\n    def __init__(self):\n        self.word2idx = {\"<PAD>\":0, \"<UNK>\":1, \"<SOS>\":2, \"<EOS>\":3}\n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n\n    def build_vocab(self, texts, vocab_size=35000):\n        print(\"Building vocabulary...\")\n        freq = Counter()\n        for i, text in enumerate(texts):\n            if i % 5000 == 0:\n                print(f\"  Processed {i}/{len(texts)}...\")\n            try:\n                words = nltk.word_tokenize(text.lower())\n            except:\n                words = text.lower().split()\n            freq.update(words)\n        \n        idx = 4\n        for w, _ in freq.most_common(vocab_size):\n            self.word2idx[w] = idx\n            self.idx2word[idx] = w\n            idx += 1\n        print(f\"âœ“ Vocabulary: {len(self.word2idx)} words\")\n\n    def encode(self, text, max_len):\n        try:\n            words = nltk.word_tokenize(text.lower())[:max_len]\n        except:\n            words = text.lower().split()[:max_len]\n        ids = [self.word2idx.get(w, 1) for w in words]\n        # Pad to max_len\n        ids = ids + [0] * (max_len - len(ids))\n        return ids\n\n    def decode(self, ids):\n        words = []\n        for idx in ids:\n            if idx in [0, 2, 3]:\n                continue\n            word = self.idx2word.get(idx, \"<UNK>\")\n            if word != \"<UNK>\":\n                words.append(word)\n        return \" \".join(words)\n\ntokenizer = HybridTokenizer()\n\n# Build vocab\nprint(\"\\nBuilding vocabulary...\")\narticles = [train_data[i]['article'] for i in range(min(12000, len(train_data)))]\nhighlights = [train_data[i]['highlights'] for i in range(min(12000, len(train_data)))]\ntokenizer.build_vocab(articles + highlights)\n\n# =============================================================================\n# MODEL ARCHITECTURE - ALL DIMENSION FIXES APPLIED\n# =============================================================================\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        # âœ… FIXED: Input = hidden_dim*2 (decoder) + hidden_dim*2 (encoder) = hidden_dim*4\n        self.attn = nn.Linear(hidden_dim * 4, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_out):\n        seq_len = encoder_out.size(1)\n        hidden_rep = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        energy = torch.tanh(self.attn(torch.cat([hidden_rep, encoder_out], dim=2)))\n        attn = torch.softmax(self.v(energy).squeeze(2), dim=1)\n        context = torch.bmm(attn.unsqueeze(1), encoder_out).squeeze(1)\n        return context, attn\n\nclass HybridSeq2Seq(nn.Module):\n    def __init__(self, vocab, embed=256, hidden=384):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, embed, padding_idx=0)\n        self.encoder = nn.LSTM(embed, hidden, batch_first=True, num_layers=2,\n                               bidirectional=True, dropout=0.3)\n        self.attn = Attention(hidden)\n        # âœ… FIXED: Decoder hidden = hidden*2\n        self.decoder = nn.LSTMCell(embed + hidden * 2, hidden * 2)\n        # âœ… FIXED: fc input = hidden*2 (decoder) + hidden*2 (context) = hidden*4\n        self.fc = nn.Linear(hidden * 4, vocab)\n        self.drop = nn.Dropout(0.3)\n\n    def forward(self, src, trg=None, max_len=60):\n        enc_emb = self.drop(self.embed(src))\n        enc_out, (h, c) = self.encoder(enc_emb)\n        \n        # âœ… FIXED: Concatenate forward + backward hidden states\n        h = torch.cat([h[-2], h[-1]], 1)\n        c = torch.cat([c[-2], c[-1]], 1)\n\n        outputs = []\n        \n        if trg is not None:\n            # Training mode\n            trg_emb = self.drop(self.embed(trg))\n            for t in range(trg.size(1)):\n                ctx, _ = self.attn(h, enc_out)\n                inp = torch.cat([trg_emb[:, t], ctx], 1)\n                h, c = self.decoder(inp, (h, c))\n                out = torch.cat([h, ctx], 1)\n                outputs.append(self.fc(out))\n            return torch.stack(outputs, 1)\n        else:\n            # Inference mode\n            inp_tok = torch.tensor([[2]] * src.size(0)).to(src.device)\n            for t in range(max_len):\n                inp_emb = self.drop(self.embed(inp_tok)).squeeze(1)\n                ctx, _ = self.attn(h, enc_out)\n                inp = torch.cat([inp_emb, ctx], 1)\n                h, c = self.decoder(inp, (h, c))\n                out = torch.cat([h, ctx], 1)\n                out = self.fc(out)\n                outputs.append(out)\n                next_tok = out.argmax(1).unsqueeze(1)\n                inp_tok = next_tok\n                if (next_tok == 3).all():\n                    break\n            return torch.stack(outputs, 1)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nâœ“ Device: {device}\")\n\nmodel = HybridSeq2Seq(len(tokenizer.word2idx)).to(device)\n\n# =============================================================================\n# DATASET - âœ… FIXED PADDING\n# =============================================================================\nclass HybridDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, i):\n        try:\n            # Access dataset item correctly\n            item = self.data[i]\n            art = item[\"article\"]\n            ref = item[\"highlights\"]\n            \n            # Preprocess article\n            proc = extractor.extract_important_sentences(art, 6)\n            \n            # Encode with proper padding\n            art_ids = tokenizer.encode(proc, 350)\n            ref_ids = tokenizer.encode(ref, 80)\n            \n            # âœ… FIXED: Proper input/target creation with padding\n            # Input: <SOS> + ref_ids (without last token)\n            inp = [2] + ref_ids[:-1]\n            # Target: ref_ids + <EOS>\n            tar = ref_ids + [3]\n            \n            # Ensure proper length\n            inp = inp[:80]\n            tar = tar[:80]\n            # Pad to 80\n            inp = inp + [0] * (80 - len(inp))\n            tar = tar + [0] * (80 - len(tar))\n            \n            return {\n                \"article\": torch.LongTensor(art_ids),\n                \"summary_input\": torch.LongTensor(inp),\n                \"summary_target\": torch.LongTensor(tar)\n            }\n        except Exception as e:\n            # Return dummy data on error\n            return {\n                \"article\": torch.zeros(350, dtype=torch.long),\n                \"summary_input\": torch.zeros(80, dtype=torch.long),\n                \"summary_target\": torch.zeros(80, dtype=torch.long)\n            }\n\ntrain_loader = DataLoader(HybridDataset(train_data), batch_size=12, shuffle=True, num_workers=0)\nval_loader = DataLoader(HybridDataset(val_data), batch_size=12, num_workers=0)\n\nprint(f\"âœ“ Training samples: {len(train_data):,}\")\nprint(f\"âœ“ Validation samples: {len(val_data):,}\")\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\nscaler = GradScaler()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n\n# =============================================================================\n# TRAINING LOOP - âœ… IMPROVED\n# =============================================================================\ndef train_one_epoch():\n    model.train()\n    total = 0\n    num_batches = 0\n    \n    for batch_idx, batch in enumerate(train_loader):\n        try:\n            src = batch[\"article\"].to(device)\n            inp = batch[\"summary_input\"].to(device)\n            tar = batch[\"summary_target\"].to(device)\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                out = model(src, inp)\n                out = out.reshape(-1, out.size(-1))\n                tar = tar.reshape(-1)\n                loss = criterion(out, tar)\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            total += loss.item()\n            num_batches += 1\n            \n            if (batch_idx + 1) % 300 == 0:\n                avg_loss = total / num_batches\n                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n        \n        except Exception as e:\n            print(f\"  Error in batch {batch_idx}: {e}\")\n            continue\n    \n    return total / max(num_batches, 1)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING (5 epochs)\")\nprint(\"=\"*60)\n\nfor e in range(5):\n    print(f\"\\nEpoch {e+1}/5:\")\n    train_loss = train_one_epoch()\n    print(f\"âœ“ Epoch {e+1} completed. Loss: {train_loss:.4f}\")\n    scheduler.step(train_loss)\n\n# =============================================================================\n# GENERATE HYBRID SUMMARY\n# =============================================================================\ndef generate_summary(article):\n    model.eval()\n    try:\n        proc = extractor.extract_important_sentences(article, 6)\n        ids = tokenizer.encode(proc, 350)\n        tens = torch.LongTensor(ids).unsqueeze(0).to(device)\n        with torch.no_grad(), autocast():\n            out = model(tens, None, 40)\n        pred = out.argmax(2)[0].cpu().numpy()\n        return tokenizer.decode(pred)\n    except:\n        return \"Summary generation error\"\n\n# =============================================================================\n# COMPUTE ROUGE FOR EXTRACTIVE (BEST BASELINE)\n# =============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING...\")\nprint(\"=\"*60)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\next_preds = []\nrefs = []\n\nfor i in range(min(200, len(val_data))):\n    if i % 50 == 0:\n        print(f\"  {i}/200 samples...\")\n    try:\n        item = val_data[i]\n        art = item[\"article\"]\n        ref = item[\"highlights\"]\n        ext = extractor.pure_extractive_summary(art)\n        ext_preds.append(ext)\n        refs.append(ref)\n    except Exception as e:\n        print(f\"  Error at sample {i}: {e}\")\n        continue\n\ndef calc_rouge(preds, refs):\n    r1 = []\n    r2 = []\n    rL = []\n    for p, r in zip(preds, refs):\n        try:\n            sc = scorer.score(r, p)\n            r1.append(sc['rouge1'].fmeasure)\n            r2.append(sc['rouge2'].fmeasure)\n            rL.append(sc['rougeL'].fmeasure)\n        except:\n            continue\n    return np.mean(r1)*100, np.mean(r2)*100, np.mean(rL)*100\n\nR1, R2, RL = calc_rouge(ext_preds, refs)\n\n# =============================================================================\n# TOP 3 HYBRID EXAMPLES (For display)\n# =============================================================================\nsamples = []\nfor i in range(min(100, len(val_data))):\n    try:\n        item = val_data[i]\n        art = item[\"article\"]\n        ref = item[\"highlights\"]\n        gen = generate_summary(art)\n        score = scorer.score(ref, gen)['rouge1'].fmeasure * 100\n        samples.append((score, gen, ref))\n    except Exception as e:\n        continue\n\nsamples.sort(reverse=True)\n\n# =============================================================================\n# FINAL OUTPUT - EXACT FORMAT\n# =============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸ“Š FINAL SUMMARY PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"ROUGE-1: {R1:.2f}%\")\nprint(f\"ROUGE-2: {R2:.2f}%\")\nprint(f\"ROUGE-L: {RL:.2f}%\")\nprint()\n\nfor i, (s, gen, ref) in enumerate(samples[:3], 1):\n    print(f\"Sample {i}:\")\n    print(f\"Generated: {gen}\")\n    print(f\"Reference: {ref}\")\n    print(\"--------------------------------------------------\\n\")\n\nprint(\"âœ“ Training and evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T05:14:44.785951Z","iopub.execute_input":"2025-11-29T05:14:44.786504Z","iopub.status.idle":"2025-11-29T06:08:24.770295Z","shell.execute_reply.started":"2025-11-29T05:14:44.786478Z","shell.execute_reply":"2025-11-29T06:08:24.769612Z"}},"outputs":[{"name":"stdout","text":"âœ“ Setup complete\n\nLoading dataset...\nâœ“ Dataset loaded\n\nBuilding vocabulary...\nBuilding vocabulary...\n  Processed 0/24000...\n  Processed 5000/24000...\n  Processed 10000/24000...\n  Processed 15000/24000...\n  Processed 20000/24000...\nâœ“ Vocabulary: 35004 words\n\nâœ“ Device: cuda\nâœ“ Training samples: 15,000\nâœ“ Validation samples: 2,000\n\n============================================================\nTRAINING (5 epochs)\n============================================================\n\nEpoch 1/5:\n  Batch 300/1250, Loss: 7.1429, Avg: 7.5278\n  Batch 600/1250, Loss: 6.7260, Avg: 7.2197\n  Batch 900/1250, Loss: 6.9312, Avg: 7.0583\n  Batch 1200/1250, Loss: 6.4669, Avg: 6.9497\nâœ“ Epoch 1 completed. Loss: 6.9357\n\nEpoch 2/5:\n  Batch 300/1250, Loss: 6.3907, Avg: 6.4519\n  Batch 600/1250, Loss: 6.4131, Avg: 6.4190\n  Batch 900/1250, Loss: 6.5062, Avg: 6.3916\n  Batch 1200/1250, Loss: 6.1708, Avg: 6.3595\nâœ“ Epoch 2 completed. Loss: 6.3537\n\nEpoch 3/5:\n  Batch 300/1250, Loss: 5.9954, Avg: 6.1389\n  Batch 600/1250, Loss: 5.8180, Avg: 6.1205\n  Batch 900/1250, Loss: 5.9433, Avg: 6.1058\n  Batch 1200/1250, Loss: 5.8871, Avg: 6.0990\nâœ“ Epoch 3 completed. Loss: 6.0978\n\nEpoch 4/5:\n  Batch 300/1250, Loss: 5.7770, Avg: 5.9598\n  Batch 600/1250, Loss: 6.2640, Avg: 5.9524\n  Batch 900/1250, Loss: 5.9604, Avg: 5.9452\n  Batch 1200/1250, Loss: 5.7880, Avg: 5.9342\nâœ“ Epoch 4 completed. Loss: 5.9335\n\nEpoch 5/5:\n  Batch 300/1250, Loss: 5.7874, Avg: 5.8425\n  Batch 600/1250, Loss: 5.9083, Avg: 5.8371\n  Batch 900/1250, Loss: 5.9086, Avg: 5.8396\n  Batch 1200/1250, Loss: 5.5615, Avg: 5.8375\nâœ“ Epoch 5 completed. Loss: 5.8354\n\n============================================================\nEVALUATING...\n============================================================\n  0/200 samples...\n  50/200 samples...\n  100/200 samples...\n  150/200 samples...\n\n============================================================\nðŸ“Š FINAL SUMMARY PERFORMANCE\n============================================================\nROUGE-1: 30.40%\nROUGE-2: 12.17%\nROUGE-L: 20.18%\n\nSample 1:\nGenerated: new : police say they are being held in the . the was arrested in the , , , . the was arrested in the , , , .\nReference: Police raided Robert Durst's Houston condo, his lawyer says .\nThe millionaire real estate heir was arrested over the weekend in New Orleans .\nHe's charged with first-degree murder in the slaying of his longtime friend in 2000 .\n--------------------------------------------------\n\nSample 2:\nGenerated: new : police say they are being held in the u.s. , police say . the was arrested in the , a . the was a , , , ,\nReference: Police in Japan say they have arrested a man, 40, after five neighbors were fatally stabbed .\nThe accused shares the same surname as the victims, aged 59 to 84, local media say .\nA police official says the man has admitted to the stabbings but refused to comment further .\n--------------------------------------------------\n\nSample 3:\nGenerated: new : the u.s. military says it will be a `` '' for the first time . the u.s. is the first time to the u.s. military in the u.s. . the is a , the\nReference: Media coverage of ISIS could spur \"real wave of Islamophobia,\" report's editor says .\nThe number of hate groups across the U.S. dropped 17% from 2013 to 2014, the report says .\nIn particular, the number of KKK chapters dropped from 163 to 72 in a year, it says .\n--------------------------------------------------\n\nâœ“ Training and evaluation complete!\n","output_type":"stream"}],"execution_count":2}]}